## 离线数仓项目介绍

### 项目背景

​		我所做的是电商大数据系统，此项目是之前公司自己的产品，做这个数仓系统的主要目的是公司为了通过该系统监测电商业务的每日交易数据，通过一定的指标计算，获取各种交易相关的综合数据，供业务部门，管理部门分析决策使用。

​		此项目采用分布式，高可用的大数据系统框架，基于成本考虑，公司采用的是Apache生态的大数据平台，各系统架构均采用集群的方式部署。

#### 项目框架

​		此项目的数据仓库的数据来源主要有两个方面，

1. ​		由前端埋点产生的用户行为数据，利用NGINX采集到日志服务器，使用flume采集日志服务器的日志文件变动，flume有source，channel，sink三个组件，此次项目上游flume采用的是taildir source，taildir支持监控多目录，断点续传，断点续传在1.7版本之后广泛使用，channel采用的是kafka channel，将采集到的数据直接发送到kafka集群，省去了sink的过程，提高了效率，在flume中还设置了拦截器，监控器，设置拦截器的目的是对采集到的数据先进行轻度的ETL，过滤掉无效数据；配置监控器主要是为了监控flume 的运行状况，如果发现尝试提交的次数远大于提交成功的次数，那么说明flume的运行状况比较差，可能是flume的内存资源不够导致的，可以尝试增加内存解决。

   ​		flume 的下游是kafka，项目中根据峰值数据量，副本数和压测的情况配置了三台kafka机器，数据保存时间设置为保存3天，副本数量为2，分配1t的硬盘，根据压测情况设置每个topic5个分区，应答机制设置为1，Leader接收到数据后就应答，消费者端的分区分配策略为range+粘性分区（尽可能均衡的随机分配），kafka内存分配为11G。

   1. producer端 缓存大小默认32M，提高到了64M

   2. batch.size 默认16k ，提高到32k

   3. linger.ms 默认0ms , 提高到20ms

   4. cosumer端 从kafka集群一批次最大抓取数据量大小默认50M，提高到80M

   5. cosumer拉取返回消息的最大条数默认500条 ，提高到800条

   6. 单条数据最大大小默认1M，提高到2M

     kafka下游是flume，source采用kafka source ，channel采用file channel ，虽然性能不是最好的，但是可以保证数据的可靠性，sink 采用hdfs sink ，将数据上传到hdfs上，在这个flume上也配置了拦截器，用来解决零点漂移问题，将checkpointdir 和backupCheckpointdir配置在不同的目录中，即使前者宕掉也可以恢复数据，在使用hdfs sink时还要解决小文件问题，设置生成文件大小与hdfs block大小一致 ，滚动时间为30分钟

     namenode与datanode通讯线程池线程数为20,保证通讯的流畅

     ​		整个数仓是建立在hive之上的，hive的数据存储在hdfs上，元数据托管在MySQL，为了保障元数据的安全，我们在keepalived做了元数据备份 ，hive的分析计算框架选择的是Spark ，运行模式是hive on Spark ，hive 将HQL 解析成Spark作业，最后由Spark完成 ，Spark的资源调度器使用的是Hadoop的yarn，hive将HQL编译为job的流程为，利用框架规则对HQL进行语法解析，然后将HQL转化为抽象语法树，然后转换成逻辑执行计划，对逻辑执行计划进行优化，然后转换成物理执行计划，对物理执行计划进行优化，最后提交到执行器执行

     ​	这是日志数据源的数据到数仓的流程

 2. 由业务系统产生的业务数据，业务数据主要分为存在MySQL中的全量表数据和增量表数据，规划好的全量表数据通过Datax全量同步采集到hdfs（由于表的数量较多，所以生成Datax配置文件的任务由一个Python脚本完成）在使用Datax同步数据时，要注意处理空值，MySQL中的null值导入hive时，应手动指定格式为'\N'，hive中的\N值导出到MySQL时，应做转义；

    增量表数据通过Maxwell采集到kafka，在由flume上传到hdfs，Maxwell的底层利用的是MySQL的主从复制原理，监控主库的binlog，将增量记录采集到kafka（使用Maxwell在首日需要用bootstrap全量同步增量表的数据）

#### 数仓设计

​	本次项目我们使用的是维度模型的星型模型，一共将数仓设计为五层，分别是ODS,DIM,DWD,DWS,ADS；

ODS层存储的主要是原始数据，根据原始数据将hive表建好之后，将hdfs的数据load进来就是ODS层，数据保持原貌不做任何修改，起到数据备份的作用，由于数据较多，采用textfile格式存储，gzip格式压缩

​	  构建数仓首先做的是**业务调研**，去熟悉业务数据和业务过程，将数据与业务过程对应起来，熟悉每个业务过程会对哪些表的数据产生影响，然后进行需求分析，查看每个指标所需的业务过程和维度是否充足。

​	  其次需要**明确数据域**，就是对数据进行纵向划分，给数据分类，便于数据的管理和应用。通常是根据业务过程划分或者部门划分，本次项目是根据业务过程划分数据域，同时，一个业务过程只能属于一个数据域。本次项目划分了5个数据域，交易域，流量域，用户域，互动域，工具域

​	  然后是**构建业务总线矩阵**，通过业务总线矩阵进行维度模型的设计，构建DIM层，DWD层	，总线矩阵一行是一个业务过程，一列是一个维度，交点表示业务过程和维度的关系，一个业务过程就是一张事务型事实表，一个维度就是一张维度表。

​	  DIM层存放维度数据，维度表有三种类型，每日全量快照表，大部分维度都适用，用日期做分区字段；例如：商品分类维度表，地区维度表，优惠券维度表。

拉链表，用来处理缓慢变化维。例如：用户维度表

​	  DWD层存放由维度外键和业务过程的度量组成的事实表，事实表分三种

事务型事实表：保存最细粒度的业务过程。例如：加购，订单，取消订单，支付成功，退单，退款等。

周期快照事实表：主要用于存量型和状态型指标。例如：购物车存量周期快照事实表。

累积快照事实表：有多个业务过程，多个时间字段，每个时间对应一个业务过程的数据。例如：确认收货累积快照事实表。

​		DIM,DWD层数据量相对于ODS不算大，所以使用orc格式存储，snappy格式压缩

在维度建模的同时，需要对数据进行一定的处理，提高数据的质量和易用性；比如数据清洗，在事实表中过滤掉关键字段为空的数据

比如脱敏，对用户维度的数据将用户隐私进行加密处理

比如维度整合，将主维表和相关维表进行维度整合，对于一些特殊字段需要进行维度退化

​	  **明确统计指标**，就是对指标进行分析，整理出指标体系继而设计出汇总模型，构建出DWS层，整理指标体系的作用就是指标定义标准化，能够避免指标定义存在歧义，指标定义重复等问题。因为绝大部分的指标都可以用原子指标，派生指标，衍生指标去定义，当统计需求较多时，必然会有统计需求对应的派生指标相同的情况，这时，我们可以将这种公共的派生指标保存下来，存放在DWS层，设计DWS层的目的就是为了减少重复计算，提高数据复用性，一张汇总表保存的是**业务过程相同，粒度相同，统计周期相同**的多个派生指标。例如：用户商品粒度订单最近1日汇总表，用户商品粒度订单最近n日汇总表,用户粒度订单历史至今汇总表等。

​		ADS层存放统计需求的数据。常见的指标有：

用户日活、周活、月活，用户留存数、留存率，用户日新增、周新增、年新增，转化率，用户流失、回流，七日内连续3天登陆、下单、收藏、支付、加购，30天内连续登陆，30天商品交易总额，品牌复购率，复购率，复购率排行，点赞、收藏、评论、领取优惠券、使用优惠券的人数，退款人数，退款率，购买商品topn，热门商品topn 等。

ADS层数据每日装载一次

在所有工作都做完之后，先在测试环境测试两遍，首先保证SQL的正确性，计算结果的准确性，其次保证脚本的正确性，在通过测试后，将脚本等打包交给项目经理，经理技术把关，经理通过了就会将系统上线





全量同步和增量同步的划分依据

根据构建的业务总线矩阵确定维度和事实，从而得出业务系统的表的同步策略，比如订单表设计为事务型事实表，每一行是一个订单事实，每产生一个订单事实业务系统的表都会产生变化，而事实表需要采集到这种变化，所以使用Maxwell对原始表进行增量同步；比如购物车状态（存量型指标）设计为周期快照事实表，采集周期为每日，所以采用datax进行全量同步。



#### 亮点解决方案

7日内连续三天登录用户数指标

```SQL
select
       '2020-06-14' dt,
       7 recent_days,
    count(distinct(user_id))   order_continuously_user_count
from
(select
    user_id,dt,
       datediff(l1,dt) two
from
(select
user_id,dt,
       lead(dt,2,null) over(partition by user_id order by dt) l1
from dws_trade_user_order_1d
where dt >= date_sub('2020-06-14',6))t1)t2
where two =2;
```

思路：先用lead开窗对日期排序再将第三日日期与第一日日期对应，然后用第三日日期减去第一日日期得到的结果为2，过滤出结果为2的数据并去重统计这些用户



#### 遇到的问题

- hdfs小文件问题

​		在flume采集日志数据向hdfs落盘时会出现大量小文件，在flume配置中设置生成文件大小与hdfs block大小一致 ，滚动时间rolltime 为5s

- flume停止脚本

- kafka数据重复，数据积压，乱序，单条数据大小

  数据重复: 精确一次消费，应答机制设置为-1，ISR最小副本数>=2 ,分区副本数>=2

  数据积压：增加分区，增加消费者数量，增加消费者CPU核数；

  ​				提高一批次从kafka集群拉取数据的数据量最大值；

  ​				提高消费者每批次从消息队列抓取数据的条数

  乱序：kafka数据总是分区内有序，分区间无序，但是服务端会缓存最近			的5个request的元数据，所以kafka能保证最近的五条数据是有序的

  单条数据大小：kafka传输的单条消息的最大值默认为1M，修改message.max.bytes  项目中修改为3M

- hive数据倾斜问题

  1. 单个key：采用两阶段聚合，第一次给相同key的数据key值前加随机数前缀，将数据分到不同的分区，然后第二次再去掉随机数前缀进行全局聚合

  2. 多个key：采样倾斜key并拆分join；

     首先通过sample算子采样计算出数据量最大的是哪几个key，然后将这几个key从RDD中拆分出来，星恒一个单独的RDD，然后给该RDD中的每条数据的key都加上随机前缀，没有数据倾斜的key对应的数据形成另一个RDD，另一个原始RDD同理，但是过滤出的个别key对应的数据要膨胀n倍，每条数据都按顺序添加随机前缀，然后将独立RDD与膨胀n倍的独立RDD进行join，此时可以将原先相同的key打散成n份，分散到多个task中去join，两个普通RDD正常join，最后将两次join的结果union起来就是最终结果

- 疑难指标：路径分析，7日内连续三天登录



   

