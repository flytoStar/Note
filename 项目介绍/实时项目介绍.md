### 实时项目介绍

​	由于公司业务的快速发展，公司开始不满足离线的t+1的时效性，开始要求更高的时效性，尤其是做活动的时候，希望能够快速看到当前的业务数据，所以我们搭建了实时数仓系统。

​		在拿到业务数据后首先进行**业务调研**，在熟悉了业务过程和数据后，将业务过程和数据对应起来，然后和业务人员核实自己的理解；然后和产品经理确认需求，进行指标分析，查看指标所需的业务过程和维度是否充足

​	在业务调研后，发现有两条业务线，一条是前端埋点产生的数据保存到日志服务器，使用flume采集日志服务器的日志文件变动，然后打到kafka集群的日志数据主题

​	另一条线是由Javaee后台产生的业务数据保存在MySQL，增量数据通过Maxwell将数据同步到kafka业务数据主题

​		接下来是**明确数据域**，就是把数据纵向划分，把数据分类，便于数据的管理和应用，我们把数据分了五个域，交易域，工具域，用户域，互动域，流量域；

交易域是加购，下单，支付，退单，退款等相关数据；

工具域是优惠券领取，优惠券使用的相关数据

用户域是注册，登录相关数据

互动域是收藏，评论相关数据

流量域是启动，曝光，页面，动作，错误

​		然后**构建业务总线矩阵**，业务总线矩阵中一行是一个业务过程，一列是一个维度，行和列的交点表示二者的关系，我们就在这个总线矩阵中描述二者的关系一行就是一张事务型事实表，一列是一张维度表

之后开始建模，首先**ods**层，我们将打入kafka的数据作为ods层，对于业务数据还要做一次首日全量同步，将历史维度数据同步到ods层

​	**DIM**层存放的是维度表，首先先从kafka中读取数据，对数据进行清洗，然后我们在设计DIM层使用了配置表，配置表中有维度表的表名，表的字段，主键等关键信息，然后通过Maxwell同步配置表中的数据，再将配置数据进行广播，然后将两条流连接在一起（connect），然后将配置信息保存在广播状态中，然后用主流中的数据的表名和广播状态中的配置信息进行匹配，如果匹配说明是维度数据，过滤掉不需要的字段，然后把维度数据向下传递，最后输出写入到PHONEIX中。

​		**DWD** 层存放的是事实表，我们将事实表输出到kafka主题中，每张事实表一个主题。对于日志数据，在从kafak中读取到之后，首先对数据进行清洗，将脏数据输出到侧输出流，然后将日志数据中的错误，启动，页面，曝光，动作，类型数据进行分流形成5张事务事实表，最后输出到kafka不同主题中；

（首先处理错误日志，如果不为空就将数据传输到侧输出流；然后处理启动日志，如果启动日志不为空，就将日志传输到侧输出流；如果不是启动日志就一定是页面日志，页面日志中有动作日志和曝光日志，将曝光日志需要的字段提取出来封装进新的json对象中，然后输出到侧输出流；将动作日志需要的字段提取出来封装进新的json对象中，然后输出到侧输出流；页面日志输出到主流）

独立访客事务事实表，首先过滤掉lastpageid不为空的数据，因为独立访客lastpageid必为空，然后按照设备id分组，再利用状态将当天已经访问过的数据过滤掉，状态中保存末次访问日期，如果状态为空或者末次访问日期不是今日，那么就是当日独立访客，然后将数据写入独立访客事实表主题中

对于业务数据，在从kafka中读取到之后，首先将对应业务过程从流中过滤出来，然后从业务系统获取相应的维度数据，再将两部分数据进行join，最后写入kafka主题中

​		比如处理加购事实表时，先从kafka主题中读取数据，然后从数据中过滤出加购表数据，再使用lookup连接器从业务数据库查出字典表，然后使用lookup join 将两张表关联起来形成加购事务事实表，最后写入加购事实主题中。

下单事务事实表，先从订单预处理表中查询数据，然后从中筛选出下单数据，然后创建kafka下单明细表

​		最后**构建指标体系**，就是对指标进行分析，整理出指标体系继而设计出汇总模型，构建出DWS层，整理指标体系的作用就是指标定义标准化，能够避免指标定义存在歧义，指标定义重复等问题。因为绝大部分的指标都可以用原子指标，派生指标，衍生指标去定义，当统计需求较多时，必然会有统计需求对应的派生指标相同的情况，这时，我们可以将这种公共的派生指标保存下来，存放在DWS层，设计DWS层的目的就是为了减少重复计算，提高数据复用性，一张汇总表保存的是**业务过程相同，粒度相同，统计周期相同**的多个派生指标。

​		最后我们将业务过程相同，粒度相同，统计周期相同的多个派生指标创建一张宽表，写入clickhouse中

​		比如商品粒度各窗口下单汇总表，将独立下单用户数，订单数创建了一张汇总表

先从kafak主题读到订单明细数据，然后去重按user_id分组统计独立下单用户数，然后分组开窗聚合，通过旁路缓存异步IO从Phoenix查询维度数据进行关联补充维度数据，最后写入clickhouse

​	最后使用SpringBoot开发了可视化接口对接sugar，根据指标在接口中完成最终的聚合统计，也就是ADS层

#### 遇到的问题

- 在做商品粒度下单窗口汇总表时，查数据库读取维度数据时遇到反压

  由于外连查询数据库本身效率比较低，再加上序列化网络传输等过程严重影响时效性造成了反压

  解决：通过增加Redis旁路缓存，所有请求先访问缓存，如果缓存命中，直接返回数据，如果未命中再查数据库再将结果写入缓存以备后用，大大提高了查询效率。

- 旁路缓存和数据一致性问题

​		写入HBASE，同时重新写入对应键值的数据到缓存

​		描述：当数据源对同一--数据进行两次修改，而旁路缓存从数据库读取数据时，第一条修改数据进入数据库，缓存读到后，第二条修改数据进入数据库，此时，缓存和数据库的数据不一致（ABBA问题）

解决：由于flink分区与kafka分区一比一对齐，所以可以使用kafka分区健，用表的主键做分区键，让相同主键的数据进入同一分区，flink的数据分区内有序，也就不会出现ABBA的问题了，然后做数据库和缓存双写

​		如果缓存失效，在读数据库和写入缓存之间发生双写，有可能出现数据库和缓存不一致的情况

​		解决：延迟双写，增加双写的间隔，睡几十毫秒，在缓存读完数据库写到缓存后再进行双写

- （地区粒度各窗口下单汇总表）

  两阶段聚合，第一阶段key前加盐值，然后keyby，开窗，聚合

  第二阶段，去掉盐值，以原来的key和endtime做keyby，聚合

主要是交易域，加购各窗口（当日加购人数），下单各窗口（当日下单人数，），支付各窗口，sku粒度下单各窗口（当日下单数，当日下单总额），地区粒度下单各窗口（各地区下单数，各地区下单总额）

#### 缓存雪崩

如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，就会造成缓存雪崩。

解决方案：

​    尽量让失效的时间点不分布在同一个时间点

#### 缓存击穿

缓存击穿，是指一个key非常热点，在不停的扛着大并发

解决方案：

设置key永不过期







ods_order_detail_inc
ods_order_info_inc
ods_order_detail_activity_inc
ods_order_detail_coupon_inc
ods_base_dic_full


将ods这五张表关联处理后形成下单事实表，然后从下单事实表中计算十日内连续下单五次的用户，先用lead开窗，把第一天日期和第五天日期放在同一行，然后用datediff将两个日期做差，然后过滤出结果为4的的数据，再去重，这些就是连续5日下单的用户，主要是用户画像衡量高价值用户来使用的，根据rfm模型，我们把30天内下单10次，10天内连续下单5次的用户定义为高价值用户30天内下单10次，但是10天内下单小于3次的定义为待挽回用户，30日内下单不到10次，单日10日内下单超过3次的定义为待发展用户



在设计dwd层时，经过分析，订单明细表和取消订单明细表的数据来源，表结构都相同，业务过程和数据状态不同，为了减少重复计算，我们把两张表公共的过程提取出来，形成订单预处理表，主要就是把订单表，订单明细表，订单明细活动表，订单明细优惠券表，字典表关联起来形成订单预处理表，处理这张表我使用的是flinksql进行处理的，处理过程中主要使用了Join，left Join，lookup Join
Join就是inner Join，leftJoin底层维护了两个状态，当左表数据来了右表数据没来，此时会出现一条左表字段有数据，右表字段对应的字段没数据，状态是insert，当右表数据来了之后会出现一条和第一条数据一样的数据，不过状态是delete，我们称之为回撤流，然后在生成一条Join成功的数据，我们在使用的时候
只需要最后Join成功的数据，所以在dws层应用时会进行过滤，我使用的是flink的pocessfunction，将时间事件最晚的那条数据过滤出来向下游传递。还有就是和字典表Join时使用的lookupjoin，lookup Join本质就是jdbc查询数据库使用时一般会设置缓存和ttl，会有弱一致性问题，一般用在数据源缓慢变化的场景，治理我们选择lookupJoin是因为字典表保存在业务数据库，lookupJoin可以直接从数据库查到，其次字典表本身可能很久才会变化一次，所以不用考虑一致性问题，在几张表关联完成后使用upsetkafkaconnector将数据写入到kafka主题中
