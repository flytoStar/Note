数据库对比

| 类型          | 特点                                       | 使用场景                           |
| :------------ | ------------------------------------------ | ---------------------------------- |
| MySQL         | 关系型数据库                               | 表之间有对应关系的数据使用         |
| Redis         | 基于内存，存储kv类型数据，查询效率高       | 数据量小，需要较高的查询效率时使用 |
| HBASE         | 对大表宽表支持较好                         | 数据量大，字段较多时使用           |
| elasticsearch | 支持倒排索引，列式存储，全局检索效率高     | 需要频繁全局检索，即席查询使用     |
| clickHouse    | 列式存储，聚合，计数汇总效率高，单表查询快 | 聚合，计数汇频繁使用               |

索引模板

```json
PUT _template/movie_test
{"index_patterns": ["movie_test*"], 
  "settings": {
    "number_of_shards": 1
  }
,
  "aliases": {
  "movie_index_1_1": {}
}
  ,
  "mappings": {
    "properties": {
      "id":{
        "type": "long"
      },
      "name":{
        "type": "text",
        "analyzer": "ik_smart"
      },
      "doubanScore":{
        "type": "double"
      },
      "actorList":{
        "properties": {
          "id":{
            "type":"long"
          },
          "name":{
            "type":"keyword"
          }
        }
      }
    }
  }
}
```

### ES读写原理

#### 写流程

​	--写操作在主shard完成后才能被复制到副本shard

​	--shard数量是固定的，不会rehash

​	客户端可能会向任意node发送写请求，如果要请求的目标文档不自在当前node，那么此node为协调节点，协调节点通过目标文档id计算目标文档是哪个node维护的，然后把请求转发到该节点，然后在该节点执行写操作，成功后再讲请求转发到其他shard所在的节点执行，都响应成功后该节点向协调节点响应执行成功，协调节点再向客户端响应执行成功。

#### 读流程

​	--读操作可以从任意shard检索文档

​	客户端可能会向任意node发送请求，如果要请求的目标文档不在当前node，那么此node为协调节点，协调节点通过目标文档id确定目标文档是哪个node维护的，然后把读请求转发给该节点，然后改节点把文档返回给昔日条节点，协调节点再把文档返回给客户端（协调节点会通过轮询的方式吧请求转发给不同的节点达到负载均衡的目的）

#### 搜索流程

​	在查询时，查询请求会广播到每个shard，每个shard会在本地搜索，然后创建一个匹配文档的优先队列，然后每个shard返回本地优先队列中文档的优先值和文档id到协调节点，协调节点合并这些值创建本地的优先队列产生全局排序的结果集，协调节点甄选出所需的文档然后向维护文档的shard发送get请求，每个shard加载出文档并返回给协调节点，当所有shard都返回成功了协调节点再向客户端返回所有取回的文档。

#### 数据物理提交流程

​	客户端将写请求提交到不可读buffer中，同时把写操作记录在内存中的translog中，然后响应客户端执行成功，1-2秒后执行refresh把不可读buffer中的数据写入整合成一个段，然后提交到可读buffer中，可读buffer在达到flush条件后写入磁盘，落盘后会周期性把段文件合并到shard文件中

### shard过多的危害

​	shard太多会占用过多CPU和内存资源，检索时多个shard同时接受请求会产生内耗

#### shard数量规划	

​	1）按照每日数据量规划  单个shard不宜超过30G

​	2）按照堆内存规划 ， 节点堆内存推荐为32G ,那么该节点的shard数量不宜超过1000个

#### shard优化

​	1）及时归档冷数据

​	2）合并分片中的段来降低分片的资源占用

#### 段的优化

segment上数据有独立的Lucene索引，如果一个shard有很多个segment，会非常影响搜索性能，还会占用大量内存

**优化** ：将多个小segment合并成大segment