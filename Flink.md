## Flink

### flink特点

​		--低延迟

​		--高吞吐

​		--高准确性和高容错率

​		--flink可以按照事件发生的时间处理数据（SparkStreaming是按照数据到达系统的时间处理数据）

​		--来一条数据处理一条

#### Flink分区

​		flink物理分区： 算子并行子任务所占用的线程数

​		flink逻辑分区：每个key对应的数据或累加器 

​		同一个逻辑分区的数据一定在一个物理分区上

### Flink架构

flink是分布式主从架构，主进程为 jobmanager 进程，从进程为 taskmanager进程

 **jobmanager** 进程（作业管理器），

​		资源管理器线程 ：管理任务插槽资源

​		分发器线程 ： webUI

​		jobmaster ： 调度任务，将有向无环图部署到任务管理器；每个任务对应一个jobmaser线程

 **taskmanager**进程（任务管理器）

​		**任务插槽**

​		任务插槽是flink最小的计算单元

​		不同算子的并行子任务可以共享同一个任务插槽

​		 相同算子的不同并行子任务不能共享同一个任务插槽

​		 算子的并行度为N，那么就有N的并行子任务，且必须占用N个任务插槽

​		taskmanager进程中有一个或多个任务插槽slot，slot是物理分区，每个slot占用一段内存，slot中至少运行一个线程

**无状态特点**

输入相同的情况下，输出一定相同



#### 逻辑分区算子

map,flatmap,filter

keyby ： 

​		 --根据key计算数据要去下游的哪个算子的哪个并行子任务

​	--相同key的数据一定会路由到相同的并行子任务

reduce： 有状态算子，将第一次出现的key作为状态变量，后来的相同key的数据来之后累计计数，更新状态变量的值后将数据丢弃



​		 

![flink任务提交流程](F:\Atguigu\04_Note\文档\MDpng\flink任务提交流程.png)

KeyedProcessFunction

每个key都维护了自己的内部状态：状态变量，定时器

**水位线 = 观察到的最大事件时间戳 - 最大延迟时间 - 1ms**

水位线到达窗口范围最大阈值-1ms触发processelement 执行，返回输出结果



多流合并

- 多条流合并成一条流会维护一个数组，保存每条流到来的水位线，每来一条数据都会找到最小的水位线更新下游算子和并行子任务的水位线

- 当前水位线是上次更新的水位线

### 迟到数据处理

-  如果窗口还在，数据就进入对应的窗口中，若窗口已经销毁，数据将被丢弃

- 通过判断，将迟到的数据发送到测输出流中去

- 使用迟到元素更新窗口数据，当水位线到达窗口结束时间触发窗口计算，单此时不销毁窗口，而是等待一段时间，把后到来的数据与之前的归并更新窗口的数据

  窗口真正销毁时间： 水位线 >= 窗口结束时间 + allowedLateness

#### 多流合并

-  union

    1.可以合并多条流

   2.  多条流类型必须一致

       

- connect
  1. 只能合并两条流
  2. 两条流类型可以不一致

- 基于间隔join

  - 当前流可以和另一条流相应的时间范围内的数据join

    ![基于间隔的join](F:\Atguigu\04_Note\文档\MDpng\基于间隔的join.png)

- 基于窗口的join

  -  将属于相同key相同窗口的数据join

    ![基于窗口的join](F:\Atguigu\04_Note\文档\MDpng\基于窗口的join.png)

Streamgraph在客户端根据程序生成有向无环图
jobgraph在客户端将Streamgraph编译优化后生成jobgraph
为了降低通信成本，合并算子链
executiongraph 在jobmanager根据jobgraph生成

遇到状态较大的作业，出现什么问题，怎么解决





#### CEP

cep允许在事件流中检测事件模式，通过一定的规则匹配，输出用户想得到的数据，数据存储在map结构中



#### 反压

程序接收数据的速率远大于程序处理的速率

**现象：**kafka数据积压，cp挂掉，甚至任务挂掉

暂时的反压可以忽略，如果是断断续续的反压必须要解决

**定位：**在 Flink Web UI 中有 BackPressure 的页面定位，红色的task任务表明出现反压了

在哪**遇到**了：1. 查数据库读取维度数据时遇到（商品粒度下单窗口汇总表）

​					解决：旁路缓存；增加并行度

​					2. 将数据写入kafka和clickhouse时出现

​					解决： 将数据攒批写入下游，减少外连的次数

​					3. 窗口聚合时出现数据倾斜时

​					

#### 数据倾斜

- keyby之前

  数据源数据本身就不均匀，可以rebanlance让flink程序强制shuffle，将数据均匀分配

- keyby之后（省份粒度各窗口下单汇总表）

  两阶段聚合，第一阶段key前加盐值，然后keyby，开窗，聚合

  第二阶段，去掉盐值，以原来的key和endtime做keyby，聚合

一条数据从source进入，在source配置发送时间，到sink输出，数据的发送时间和sink输出后的本地系统时间的差值就是全链路延迟，数值根据程序的复杂性的不同而不同，一般在30ms左右

#### flinksql优化

localglobal两阶段聚合：第一阶段在本地将数据攒批聚合，第二阶段再进行合并

使用localglobal需要开启minibatch

#### 遇到的其他问题

- 旁路缓存和数据库的一致性问题

​		描述：当数据源对同一数据进行两次修改，而旁路缓存从数据库读取数据时，第一条修改数据进入数据库，缓存读到后，第二条修改数据进入数据库，此时，缓存和数据库的数据不一致

- 分区没有数据，waterMark不产生，窗口不能创建的问题

​			**分区为什么没数据**  

​			 kafka分区数小于flink并行度 ，将并行度和kafka分区数设置为1：1；或者设置最大空闲等待时间

​			waterMark注入的位置太靠下，上游因为重分区或者过滤的原因导致没数据，将waterMark注入的位置上移解决



#### flink精确一次

​	由于flink上游是kafka，kafka作为source，将offset保存在状态中，即使程序挂掉也能从检查点恢复offset，能够保障精确一次。

​	flink内部，checkpoint机制，会把检查点保存下来，程序故障重启后可以恢复

​	sink到kafka实现了两阶段提交，第一次数据写入kafka后进行预提交，第二次是checkpoint保存完才正式提交，预提交失败可以回滚；

如果下游是不支持事务的介质，可以用幂等性实现，HBASE的rowkey具有唯一性，是幂等的；clickhouse使用replacingmergeTree去重，查询时加final实现幂等性



#### 窗口

- 分类

  - 时间窗口：滑动，滚动
  - 计数窗口：滑动，滚动
  - 会话窗口

- 组成

  分配器，触发器，驱逐器，处理函数

- 生命周期

  创建：第一条数据到达

  销毁：事件时间到达窗口结束时间+允许超时时间

- 触发

​		waterMark大于窗口结束时间-1ms

#### flink状态

算子状态：算子每个并行子任务维护一个状态

键控状态：每个并行子任务的分组维护一个状态

​				自定义状态：独立访客统计，将每个mid的最后一次访问日期保存在状态中

​				regular join中的状态

状态后端：memory 基于内存

​				rocksDB  基于内存和磁盘，可以指定磁盘类型套餐

#### 时间语义

- 事件时间
  - 数据上的时间，用来触发窗口，定时器；用在join和cep
- 处理时间
  - 算子计算的系统时间
- 摄入时间
  - 进入flink的时间

#### barrier对齐

- barrier对齐

  并行子任务barrier到达快的会把数据缓存到状态中，不做处理，等其他并行子任务的barrier到达后，该算子才保存检查点

  优点：状态后端保存的数据少

  缺点：延迟高，会阻塞数据的处理

- barrier不对齐

  并行子任务barrier到达快的会正常计算，其他到达慢的子任务会再barrier到达后进行快照，进行快照时会计算一部分后到来的数据，然后把计算完的数据保存到状态中

  优点：延迟低，不会阻塞数据

  缺点：会造成数据重复消费

#### flink join

- 维度join

  dws层和维度数据join

  - lookup join：这种关联要求一张表（主表）有处理时间字段，而另一张表（维表）由 Lookup 连接器生成。加购表和字典表join时使用了，本质是利用jdbc查数据库，一般使用会设置缓存，会有弱一致性问题，ttl周期内数据库变化，但是缓存没变（使用在数据源缓慢更新的场景）

  - 旁路缓存

    为什么用HBASE，因为HBASE扩展性更强，支持的数据量更大，团队比较熟悉

    - 旁路缓存和数据库的一致性问题

      写入HBASE，同时重新写入对应键值的数据到缓存

    ​		描述：当数据源对同一数据进行两次修改，而旁路缓存从数据库读取数据时，第一条修改数据进入数据库，缓存读到后，第二条修改数据进入数据库，此时，缓存和数据库的数据不一致（ABBA问题）

    解决：由于flink分区与kafka分区一比一对齐，所以可以使用kafka分区健，用表的主键做分区键，让相同主键的数据进入同一分区，flink的数据分区内有序，也就不会出现ABBA的问题了，然后做数据库和缓存双写

    - 如果缓存失效，在读数据库和写入缓存之间发生双写，有可能出现数据库和缓存不一致的情况

      解决：延迟双写，增加双写的间隔，睡几十毫秒，在缓存读完数据库写到缓存后再进行双写

    - 缓存预热

      会规划一部分相对较热的数据提前写入缓存，比如三天内的访客，或者热门商品，做活动的商品

  - temporal join ： 对维度时效非常敏感的场景，比如汇率

- 流join

  - window join ：本质是inner join可以利用watermark解决迟到数据join不上的问题
  
  - window cogroup connect ： 一般用来left join /right join
  
  - interval join ： 当前流可以和另一条流相应的时间范围内的数据join
  
    对两条流中拥有相同键值以及彼此之间时间戳不超过某一指定间 隔的事件进行 Join。
  
  - regular join ： flinksql的join ，利用状态缓存了数据（支付成功事实表，join支付表和字典表）
  
  API：window Join ； cogroup connect ； intervalJoin
  
  sql：regular Join ； lookup Join ；  temporal join

#### flink解决双流join数据迟到问题

- watermark
- 侧输出流
- 窗口设置数据允许迟到时间
- 状态的ttl
